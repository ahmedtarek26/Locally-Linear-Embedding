# Locally Linear Embedding (LLE) - Implementation & Comparison

Complete from-scratch implementation of LLE with comparison to course lab methods.

## ðŸ“„ Paper Reference

**Roweis, S. T., & Saul, L. K. (2000).** *Nonlinear dimensionality reduction by locally linear embedding.* Science, 290(5500), 2323-2326.

## ðŸŽ¯ Project Overview

This project demonstrates deep understanding of manifold learning through:

1. **Complete LLE implementation from scratch** - No sklearn.manifold.LocallyLinearEmbedding
2. **Course lab code reuse** - Explicit use of methods from UL25 course labs
3. **Systematic comparison** - LLE vs 6 methods from Unsupervised Learning course
4. **Swiss Roll experiment** - Classic benchmark dataset from the LLE paper

## ðŸ—ï¸ Repository Structure

```
.
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ lle_implementation.py              # LLE from scratch
â”œâ”€â”€ course_methods.py                  # Lab 2-5 method wrappers
â”œâ”€â”€ comparison_framework.py            # Comparison utilities
â”œâ”€â”€ run_experiment.py                  # Main experiment script
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ LLE_theory.md                  # Mathematical derivation
â”‚   â””â”€â”€ dataset_info.md                # Swiss Roll dataset details
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ swiss_roll_data.npy            # Original dataset
â”‚   â”œâ”€â”€ swiss_roll_color.npy           # Ground truth colors
â”‚   â””â”€â”€ swiss_roll_scaled.npy          # Standardized data
â””â”€â”€ results/
    â”œâ”€â”€ embedding_*.csv                # 2D embeddings from each method
    â””â”€â”€ comparison_summary.csv         # Timing comparison table
```

## ðŸ“š Course Context

**Course:** Unsupervised Learning 2025  
**Institution:** University of Trieste  
**Professor:** Alejandro Rodriguez Garcia  
**Lab Instructor:** Francesco Tomba  
**Course GitHub:** https://github.com/lykos98/UL25

### Lab Methods Used

This project explicitly reuses code patterns from course labs:

- **Lab 2** (`Lab2-PCA.ipynb`): PCA, Classical MDS
- **Lab 3** (`Lab3-Isomap.ipynb`): Isomap
- **Lab 4** (`Lab4-KernelPCA.ipynb`): Kernel PCA  
- **Lab 5** (`Lab5-DimensionalityReduction.ipynb`): t-SNE, UMAP

All methods in `course_methods.py` mirror the implementations from these labs.

## ðŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/lle-project.git
cd lle-project

# Create directories
mkdir -p data results

# Install dependencies
pip install -r requirements.txt
```

## ðŸš€ Quick Start

### Run Full Experiment

```bash
python run_experiment.py
```

This will:
1. Generate Swiss Roll dataset (2000 points, 3Dâ†’2D)
2. Run LLE and all 6 course lab methods
3. Save embeddings and timing results

### Use LLE in Your Code

```python
from lle_implementation import LocallyLinearEmbedding
from sklearn.datasets import make_swiss_roll

# Generate data
X, color = make_swiss_roll(n_samples=2000)

# Apply LLE
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=12)
Y = lle.fit_transform(X)

print(f"Reduced from {X.shape[1]}D to {Y.shape[1]}D")
```

### Use Course Lab Methods

```python
from course_methods import CourseLabMethods
from sklearn.preprocessing import StandardScaler

# Standardize data
X_scaled = StandardScaler().fit_transform(X)

# Apply Lab 3 Isomap
lab_methods = CourseLabMethods()
result = lab_methods.lab3_isomap(X_scaled, n_components=2, n_neighbors=12)
Y_isomap = result['embedding']
```

## ðŸ”¬ Methods Compared

| Method | Source | Type | Key Feature |
|--------|--------|------|-------------|
| **LLE** | Our Implementation | Nonlinear (local) | Preserves local linear reconstructions |
| **PCA** | Lab 2 | Linear | Maximizes variance |
| **MDS** | Lab 2 | Distance-based | Preserves pairwise distances |
| **Isomap** | Lab 3 | Nonlinear (global) | Preserves geodesic distances |
| **Kernel PCA** | Lab 4 | Nonlinear (kernel) | PCA in feature space |
| **t-SNE** | Lab 5 | Nonlinear (probabilistic) | Preserves probability distributions |
| **UMAP** | Lab 5 | Nonlinear (topological) | Graph-based manifold learning |

## ðŸ“Š Results on Swiss Roll

### Timing Comparison

| Method | Time (s) | Source | Type |
|--------|----------|--------|------|
| PCA | 0.011 | Lab 2 | linear |
| KernelPCA | 0.179 | Lab 4 | nonlinear_kernel |
| **LLE** | **1.289** | **Our Implementation** | **nonlinear_local** |
| Isomap | 2.324 | Lab 3 | nonlinear_global |
| MDS | 11.285 | Lab 2 | distance_based |
| t-SNE | 14.404 | Lab 5 | nonlinear_probabilistic |
| UMAP | 14.997 | Lab 5 | nonlinear_topological |

### Key Findings

âœ… **LLE is competitive**: 6-12Ã— faster than t-SNE/UMAP  
âœ… **Good quality**: Successfully unrolls Swiss Roll manifold  
âœ… **One parameter**: Only K (number of neighbors) needs tuning  
âœ… **No local minima**: Eigenvalue problem has global solution

## ðŸ“Š Dataset: Swiss Roll

The Swiss Roll is a 2D manifold embedded in 3D space, widely used for benchmarking manifold learning algorithms.

**Specifications:**
- N = 2,000 points
- Embedded in 3D space
- Intrinsic dimension = 2D
- Noise = 0.1
- Same as used in LLE paper (Roweis & Saul, 2000)

**Why Swiss Roll?**
- Tests ability to "unroll" nonlinear structure
- Points close on the surface may be far in 3D Euclidean space
- Linear methods (PCA) fail - can't unroll it
- Manifold methods (LLE, Isomap) succeed - preserve local geometry

See `docs/dataset_info.md` for full details.

## ðŸ§® LLE Algorithm

### Three Steps

1. **Find Neighbors**: For each point, find K nearest neighbors
2. **Compute Weights**: Minimize reconstruction error
   ```
   Îµ(W) = Î£áµ¢ ||Xáµ¢ - Î£â±¼ Wáµ¢â±¼ Xâ±¼||Â²
   ```
   Subject to: Î£â±¼ Wáµ¢â±¼ = 1, Wáµ¢â±¼ = 0 if j âˆ‰ neighbors(i)

3. **Compute Embedding**: Minimize embedding cost
   ```
   Î¦(Y) = Î£áµ¢ ||Yáµ¢ - Î£â±¼ Wáµ¢â±¼ Yâ±¼||Â²
   ```
   Solution: bottom d+1 eigenvectors of M = (I - W)áµ€(I - W)

See `docs/LLE_theory.md` for complete mathematical derivation.

## ðŸ“– Documentation

- **[LLE_theory.md](docs/LLE_theory.md)**: Complete mathematical derivation with all equations
- **[dataset_info.md](docs/dataset_info.md)**: Detailed Swiss Roll dataset information

## ðŸŽ“ What This Project Demonstrates

1. âœ… **Deep understanding** of LLE mathematics
2. âœ… **Implementation skills** - from-scratch Python code
3. âœ… **Course integration** - explicit reuse of lab code
4. âœ… **Experimental rigor** - systematic comparison
5. âœ… **Documentation** - professional-grade writeup

## ðŸ“ˆ When to Use LLE

### Best For
- Smooth, well-sampled manifolds
- Moderate datasets (N = 1,000-10,000)
- When local geometry preservation matters
- When computational efficiency is important

### Not Ideal For
- Multiple disconnected components
- Very large datasets (N > 100,000) - use UMAP
- Complex topology (holes, handles)
- When you need parametric mapping

## ðŸ“š References

1. Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear embedding. *Science*, 290(5500), 2323-2326.

2. Tenenbaum, J. B., De Silva, V., & Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. *Science*, 290(5500), 2319-2323. (Isomap)

3. van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. *Journal of Machine Learning Research*, 9, 2579-2605.

4. McInnes, L., Healy, J., & Melville, J. (2018). UMAP: Uniform manifold approximation and projection. *arXiv preprint* arXiv:1802.03426.

5. Course materials: https://github.com/lykos98/UL25

## ðŸ“œ License

MIT License - see LICENSE file for details.

## ðŸ‘¤ Author

Unsupervised Learning Course Project 2025  
University of Trieste

---

**Note:** This is an educational project implementing the LLE algorithm from scratch and comparing it with methods from the Unsupervised Learning course labs.
